# Methods

```{r}
#| label: libraries
#| include: false

library(gt)
library(lubridate)
library(tidyverse)

```

The study will rely on available data to develop regression and classification models to predict native vegetation suppression and its impacts. 
To achieve the proposed goal, I will divide the work into five major parts:

1.  Literature overview of the main d rivers of the expected outcomes (vegetation suppression, land cover transition, social and environmental variables);

2.  Experts interviews;

2.  Screening for available data and development of structured data collection;

3.  Development of models;

4.  Models interpretations;

```{mermaid}
%%| label: fig-method-diagram
%%| fig-cap: Workflow of the project
%%| fig-responsive: true
%%| fig-align: center

flowchart TD
    A[Literature Review] --> B[Available Data]
    A[Literature Review] --> C[Database structuring]
    B[Available Data] --> C[Database structuring]
    D[Experts interviews] --> C[Database structuring]
    C[Database structuring] --> E[Models Development]
    A[Literature Review] --> E[Models Development]
    D[Experts interviews] --> E[Models Development]
    E[Models Development] --> G[Models Interpretations]

```

\

The literature review will provide a theoretical and practical basis for the modeling section. 
Information about the key variables, their manipulation process, and the analysis techniques used to reach the results.

The database structuring will store all the relevant data in a single file structure to facilitate the modeling process.

The modeling process will follow standard procedures and libraries, with the possibility to implement different techniques.

Models interpretation will be performed to understand the importance of the variables and their interactions, and to discuss public policy planning to mitigate the negative impacts of native vegetation suppression in the Amazon and Cerrado.

## Literature Review

The literature review will address different topics. 
For all of them, I will adopt a hybrid approach of database searches combined with snowball searches.

```{mermaid}
%%| label: fig-review-diagram
%%| fig-cap: Workflow of literature review
%%| fig-responsive: true
%%| fig-align: center

flowchart TD
    A[Specific Questions] --> B[Query Development]
    A[Specific Questions] --> C[Documents Screening]
    B[Query Development] --> D[Database Search]
    D[Database Search] --> C[Documents Screening]
    C[Documents Screening] --> E[Documents Collection]
    E[Documents Collection] --> F[Forward Snowball Search]
    F[Forward Snowball Search] --> C[Documents Screening]
    E[Documents Collection] --> G[Final Collection]

```

\

The literature review will answer questions about variables to be predicted in this project. 
For each variable, I want to explore the main drivers that cause them, how they relate to each other, and the methods used to estimate the effects and interactions. 

## Experts Interviews

The objective of the interviews is to explore a diverse range of possible causes of deforestation, and its impacts on the environment and society.

The target participants are researchers with past experience on analyzing deforestation in the Amazon or Cerrado biomes.

The prospection of potential participants will be done by snowball sampling.

The interviews will consist of open questions and an interactive activity of building a Directed Acyclic Graph (DAG) with the participants.

### Questions

The open questions will serve to develop a context of the participant experience and perspectives on deforestation:

 - What were your past experiences on deforestation?
 
 - What were the regions that you developed projects related to deforestation?
 
 - What is your perspective on deforestation, taking in account sustainability and human development?
 
 - Based on your experience, what are the main conditions that facilitate the occurrence of deforestation?
 
### Interactive activity

The objective of developing DAGs with different participants is to explore a diversity of possibilities to model deforestation. 
The use of DAGs before any modelling exercise (such as feature selection) is useful to reflect about the phenomena, explore potential variables, look for possible confounders, colliders and mediators, and finally choose an appropriate model.

It is also possible to compare DAGs (since they are graphs!), so it will open an opportunity to explore convergences and divergences between participants.

The DAGs will be create during the interview with the use of a interactive tool.

## Database Structuring

The database will contain data from different domains, which can be available in diverse formats and structures.
To make the modeling process more efficient and transparent, all variables of interest must be organized coherently.

The process to create the database will consist of accessing the data sources, downloading the data to the local environment, transforming and organizing the data to the grid (if the data source provides any pre-processing tool before downloading the data, it should be done that way), and store in the local environment.

```{mermaid}
%%| label: fig-data-diagram
%%| fig-cap: Mermaid diagram
%%| fig-responsive: true
%%| fig-align: center

flowchart TD
    A[Data Source] --> E[Pre Processing]
    B[Base Grid] --> E[Pre Processing]
    E[Pre Processing] --> F[Gridded Data]
    F[Gridded Data] --> G[Database]

```

The data will be stored in a nested directories database with hive-style partitioning. 
The database will be composed of a collection of parquet files, a format with high compression, that facilitates processing large volumes of data.
The main partition will be the variables, which means that each variable will be stored in a separate directory.
If necessary, the data will be partitioned by further variables (as a rule of thumb, each parquet file should not be smaller than 20MB and not bigger than 2GB).

### Base grid

To structure a collection of data that presents different natures, I will transform all the variables into a standard grid system.
The grid system will be composed of 0.2 degrees cells (40km x 40km), which should be a balanced compromise between detail and computational cost.

```{r}
#| label: fig-base-grid
#| fig-cap: "Model grid, in which variables were transformed to"
#| fig-asp: 1
#| echo: false

load("./figs/base_grid.rdata")

viz_grid

```

Each grid cell will contain an observation, for each year, for all the variables. 
Only pixels that contained natural vegetation in the first year of the analysis will be considered valid.

### Variables

The variables used in this project will serve to predict deforestation and its impacts on social and environmental dimensions.
The variables may present diverse nature, such as quantitative and qualitative.
A major part of this project will be figuring out the most appropriate way to translate these variables in an organized matrix that will be supplied to the models.

```{r}
#| label: tbl-variables
#| tbl-cap: Variables used in this project.
#| echo: false

htmltools::includeHTML("./figs/variables_table.html")

```

#### Biomes

The biomes limits are provided by IBGE.
The extent of the biomes have been changing over the years, and since we are working on a time series, the extent will dependent on the year.

The downloaded data was filtered to contain only the limits of the Amazon and Cerrado biomes.

#### Deforestation

The deforestation variable is derived from the MapBiomas land use and land cover collection.

The downloaded data was cropped to the extent of the Cerrado and Amazon biomes.
Than the raster data was aggregated to a coarser spatial resolution, from 30 to 300 meters, assigning the value of the most frequent class to the new pixel (the mode of the 100 pixels that will be aggregated to the 300 meters pixels).
This was done to reduce the amount of observations, which would be unfeasible to process, since the original data consists of hundreds of billions pixels.

#### Conservation Units

The limits of the Conservation Units (UC) are provided by the Ministry of Environment and Climate Change. These are the conservation units that finished the registration process at the National Register of Conservation Units (CNUC). However, the data used in this project is accessed by the Institute for Applied Economic Research (IPEA), which organizes data in a standard attributes and spatial projection. The data is represented as spatial vectors.

The UC vectors are intersected to the base grid, and the area of each UC inside each cell is calculated.

#### Indigenous Lands

The limits of Indigenous Lands (IL) inside the Amazon and Cerrado are provided by FUNAI.

## Model Development

The models will follow a sequential order.
The output of a model will serve as a feature of the next model.

```{r}
#| label: model-table
#| echo: false

table <-
  tibble(
    section = c(
      "Vegetation suppression probability", "Vegetation suppression area", 
      "Land cover transitions", "Transitions impacts"
    ),
    model_type = c(
      "Classification", "Regression",
      "Classification", "Regression"
    )
  )

table %>%
  pivot_wider(names_from = section, values_from = model_type) %>%
  gt() %>%
  cols_align(align = "center") %>%
  tab_options(
    table.border.top.style = "hidden",
    table.border.bottom.style = "hidden"
  )

```

The first model will classify areas that suffered vegetation suppression.
So the outcome will be a binary result (0 = no suppression; 1 = suppression).
I want this model to have the minimum inclusion errors for class "1".
In other words, I want to avoid classifying an area with the label "1" when the reality is that it should be labeled "0".
To achieve this, the model will be adjusted to present the minimum inclusion error for class "1", and the probability threshold to classify an observation as class "1" will be a higher value.

The second model will predict the amount of vegetation suppression that was classified with the label "1" in the previous model.
The predictions will have to be bounded by the amount of natural vegetation inside the grid cell (a Poisson distribution may be adequate to represent this variable).

The third model will predict the predominant land use transitions of the areas that suffered native vegetation suppression.

The fourth model will be a set of models.
Each model will be responsible to predict a single environmental or social outcome.

Each model will follow the standard approach to develop supervised models.

```{mermaid}
%%| label: fig-model-diagram
%%| fig-cap: Mermaid diagram
%%| fig-responsive: true
%%| fig-align: center

flowchart TD
    A[Database] --> B[Variables Selection]
    B[Variables Selection] --> C[Data Split]
    C[Data Split] --> D[Training Data]
    D[Training Data] --> E[Pre Processing]
    E[Pre Processing] --> F[Initial Model Fit]
    C[Data Split] --> G[Validation Data]
    C[Data Split] --> H[Test Data]
    F[Initial Model Fit] --> I[Parameters Adjustments]
    G[Validation Data] --> I[Parameters Adjustments]
    I[Parameters Adjustments] --> J[Final Model Fit]
    H[Test Data] --> K[Model Assessment]
    J[Final Model Fit] --> K[Model Assessment]
    J[Final Model Fit] --> L[Model Interpretation]
    K[Model Assessment] --> L[Model Interpretation]

```

\

To understand and interpret the predictions made by the models, I will employ a variety of methods. They may include surrogate models, feature interactions, permutation feature importance, counterfactual explanations, and Shapley values. 
These methods are model-agnostic, meaning they can be applied to any regression or classification model, regardless of its underlying structure or parameters.
By using these techniques, I aim to gain a deeper understanding of the relationships between the variables and how they contribute to the predictions.
This information will enable assessing the reliability of the models and validity of their results.
