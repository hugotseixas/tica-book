# Methods {#sec-methods}

```{r}
#| label: libraries
#| include: false

library(gt)
library(lubridate)
library(tidyverse)

```

This project relies on developing a theoretical model of how and why Natural Vegetation Suppression (NVS) occurs persistently in the Amazon and Cerrado. This theoretical model is responsible to represent the mechanisms that causes the NVS. Based on the theoretical model, sociological and statistical analysis are performed to describe the causes of NVS from subjective and objective perspectives.

To employ the tasks above and accomplish the objectives (see @sec-objectives), the project is divided into four parts:

1.  Creation of the NVS theoretical model;

2.  Sociological assessment of agents related to the NVS theoretical model;

3.  Statistical analysis of data related to the NVS theoretical model;

4.  Interpretation of the results;

The first part is performed by conducting an adaptation of the Critical Realist Grounded Theory [@oliver-2011] method (which is itself an adaptation of the general Grounded Theory method). My adaptation combines the interactive engagement with data trough the aquisition and coding process, and the retroduction aproach to reflect about causal mechanisms of NVS. The reult of this process will be a theoretic model, represented by a Directed Acyclic Graph (DAG), which abstracts relationships of cause between variables.

In the second part, a sociological analysis describes how groups of interest are related to the variables from the NVS theoretical model, and what are the interactions between different groups, according to their actions and power status. The analysis will be based on Fields Theory of Bourdieu.

The third part contains the development and employment of statistical regression algorithms, that will be trained and tested using available data that are represented in the NVS theoretic model. The prediction of the models will be analysed by machine learning interpretation techniques.

The fourth and last part is where results of both aproaches (sociological and statistical analyses) will be interpreted in conjunction.

The sequence of activities that compose each part are represented in @fig-method-diagram, summarising the method.

```{mermaid}
%%| label: fig-method-diagram
%%| fig-cap: "General workflow of the method. The activities that composes each part of the project are: (1) Literature Assessment, Experts Interviews, Theoretical Model; (2) Sociological Analysis; (3) Available Data, Database Structuring, Statistical Analysis; (4) Results Interpretation."
%%| fig-responsive: true
%%| fig-align: center

flowchart TD
    A[Literature Assessment] --> B[Theoretical Model]
    C[Experts Interviews] --> B
    B --> D[Database Structuring]
    E[Available Data] --> D
    B --> F[Sociological Analysis]
    C --> G[Statistical Analysis]
    D --> G
    F --> H[Results Interpretation]
    G --> H

```

The combination of activities configures as a Convergent Parallel Mixed Method of research, where quantitative and qualitative analysis are perform at the same time, providing results to a final interpretation, However, the design of this work includes a previous establishment of a common theory that guides both analysis, so they share the same foundations.

In the next sections, each part of the work, and its respective activities are described.

## Theoretical Model

The theoretical model will be created by the employment of an adaptation of the Critical Realist Grounded Theory. The method of Grounded Theory is widely used in social sciences with the objective of generating theories, it consists of an interactive and concurrent process of data collection and analysis. It have been modifyed trough time to accomodate different paradigms of research, such as the Critical Realism.

The Critical Realist Grounded Theory introduces the necessity to explicitly account for the link between observations and their underlying generative mechanisms, and the use of retroduction in order to abstract these mechanisms [@oliver-2011].

The process of data aquisition is performed by a series of interviews and literature assessment. The interviews inquire possible causes of NVS and the social groups related to this phenomena. After each interview, the answers are coded into general terms, and with related question of how and why they can affect NVS. These questions are transformed into literature database queries, that helps finding written material that may enrich analysis with convergences or divergences with interview answers, or with new causes of NVS. After each round of interviews and literature assessment, the next interview may change to accomodate findings from the previous. This cyclic process is demonstrated in @fig-theoretical-diagram.

```{mermaid}
%%| label: fig-theoretical-diagram
%%| fig-cap: Workflow of theoretical model development
%%| fig-responsive: true
%%| fig-align: center

flowchart TD
    A[Interview] --> B[Data]
    C[Questions] --> D[Literature Assessment]
    D --> E[Data]
    B --> F[Theoretical Model]
    F --> C
    E --> F
    F --> A
 
```

This process is repeated until the data from interviews and literature assessment satrts to get redundant, adding little additional information to the theoretical model.

### Experts Interviews

The objective of the interviews is to explore a diverse range of possible causes of deforestation, and its impacts on the environment and society.

The target participants are researchers with past experience on analyzing deforestation in the Amazon or Cerrado biomes.

The prospecting of potential participants are done by snowball sampling.

The interviews consist of open and closed questions (see @sec-interview-schedule for the interview schedule), and an interactive activity of building a Directed Acyclic Graph (DAG) with the participants (see @sec-interview-dag).

### Literature Assessment

The literature review addresses different topics. For all of them, I adopt a hybrid approach of database searches combined with snowball searches.

```{mermaid}
%%| label: fig-review-diagram
%%| fig-cap: Workflow of literature review
%%| fig-responsive: true
%%| fig-align: center

flowchart TD
    A[Questions] --> B[Query Development]
    A[Questions] --> C[Documents Screening]
    B[Query Development] --> D[Database Search]
    D[Database Search] --> C[Documents Screening]
    C[Documents Screening] --> E[Documents Collection]
    E[Documents Collection] --> F[Forward Snowball Search]
    F[Forward Snowball Search] --> C[Documents Screening]
    E[Documents Collection] --> G[Final Collection]

```

The literature review will answer questions about variables to be predicted in this project. For each variable, I want to explore the main drivers that cause them, how they relate to each other, and the methods used to estimate the effects and interactions.

## Sociological Analysis

## Statistical Analysis

### Database Structuring

The database will contain data from different domains, which can be available in diverse formats and structures. To make the modeling process more efficient and transparent, all variables of interest must be organized coherently.

The process to create the database will consist of accessing the data sources, downloading the data to the local environment, transforming and organizing the data to the grid (if the data source provides any pre-processing tool before downloading the data, it should be done that way), and store in the local environment.

```{mermaid}
%%| label: fig-data-diagram
%%| fig-cap: Mermaid diagram
%%| fig-responsive: true
%%| fig-align: center

flowchart TD
    A[Data Source] --> E[Pre Processing]
    B[Base Grid] --> E[Pre Processing]
    E[Pre Processing] --> F[Gridded Data]
    F[Gridded Data] --> G[Database]

```

The data will be stored in a nested directories database with hive-style partitioning. The database will be composed of a collection of parquet files, a format with high compression, that facilitates processing large volumes of data. The main partition will be the variables, which means that each variable will be stored in a separate directory. If necessary, the data will be partitioned by further variables (as a rule of thumb, each parquet file should not be smaller than 20MB and not bigger than 2GB).

To structure a collection of data that presents different natures, I will transform all the variables into a standard grid system. The grid system will be composed of 0.2 degrees cells (40km x 40km), which should be a balanced compromise between detail and computational cost.

```{r}
#| label: fig-base-grid
#| fig-cap: "Model grid, in which variables were transformed to"
#| fig-asp: 1
#| echo: false

load("./figs/base_grid.rdata")

viz_grid

```

Each grid cell will contain an observation, for each year, for all the variables. Only pixels that contained natural vegetation in the first year of the analysis will be considered valid.

### Model Development

The models will follow a sequential order. The output of a model will serve as a feature of the next model.

```{r}
#| label: model-table
#| echo: false

table <-
  tibble(
    section = c(
      "Vegetation suppression probability", "Vegetation suppression area", 
      "Land cover transitions", "Transitions impacts"
    ),
    model_type = c(
      "Classification", "Regression",
      "Classification", "Regression"
    )
  )

table %>%
  pivot_wider(names_from = section, values_from = model_type) %>%
  gt() %>%
  cols_align(align = "center") %>%
  tab_options(
    table.border.top.style = "hidden",
    table.border.bottom.style = "hidden"
  )

```

The first model will classify areas that suffered vegetation suppression. So the outcome will be a binary result (0 = no suppression; 1 = suppression). I want this model to have the minimum inclusion errors for class "1". In other words, I want to avoid classifying an area with the label "1" when the reality is that it should be labeled "0". To achieve this, the model will be adjusted to present the minimum inclusion error for class "1", and the probability threshold to classify an observation as class "1" will be a higher value.

The second model will predict the amount of vegetation suppression that was classified with the label "1" in the previous model. The predictions will have to be bounded by the amount of natural vegetation inside the grid cell (a Poisson distribution may be adequate to represent this variable).

The third model will predict the predominant land use transitions of the areas that suffered native vegetation suppression.

The fourth model will be a set of models. Each model will be responsible to predict a single environmental or social outcome.

Each model will follow the standard approach to develop supervised models.

```{mermaid}
%%| label: fig-model-diagram
%%| fig-cap: Mermaid diagram
%%| fig-responsive: true
%%| fig-align: center

flowchart TD
    A[Database] --> B[Variables Selection]
    B[Variables Selection] --> C[Data Split]
    C[Data Split] --> D[Training Data]
    D[Training Data] --> E[Pre Processing]
    E[Pre Processing] --> F[Initial Model Fit]
    C[Data Split] --> G[Validation Data]
    C[Data Split] --> H[Test Data]
    F[Initial Model Fit] --> I[Parameters Adjustments]
    G[Validation Data] --> I[Parameters Adjustments]
    I[Parameters Adjustments] --> J[Final Model Fit]
    H[Test Data] --> K[Model Assessment]
    J[Final Model Fit] --> K[Model Assessment]
    J[Final Model Fit] --> L[Model Interpretation]
    K[Model Assessment] --> L[Model Interpretation]

```

To understand and interpret the predictions made by the models, I will employ a variety of methods. They may include surrogate models, feature interactions, permutation feature importance, counterfactual explanations, and Shapley values. These methods are model-agnostic, meaning they can be applied to any regression or classification model, regardless of its underlying structure or parameters. By using these techniques, I aim to gain a deeper understanding of the relationships between the variables and how they contribute to the predictions. This information will enable assessing the reliability of the models and validity of their results.

# References {.unnumbered}