{
  "hash": "3e6a219b6f680f847ade9708b5b9fa66",
  "result": {
    "markdown": "# Methods {#sec-methods}\n\n\n\n\n\nThis project relies on developing a theoretical model of how and why Natural Vegetation Suppression (NVS) occurs persistently in the Amazon and Cerrado. This theoretical model is responsible to represent the mechanisms that causes the NVS. Based on the theoretical model, sociological and statistical analysis are performed to describe the causes of NVS from subjective and objective perspectives.\n\nTo employ the tasks above and accomplish the objectives (see @sec-objectives), the project is divided into four parts:\n\n1.  Creation of the NVS theoretical model;\n\n2.  Sociological assessment of agents related to the NVS theoretical model;\n\n3.  Statistical analysis of data related to the NVS theoretical model;\n\n4.  Interpretation of the results;\n\nThe first part is performed by conducting an adaptation of the Critical Realist Grounded Theory [@oliver-2011] method (which is itself an adaptation of the general Grounded Theory method). My adaptation combines the interactive engagement with data trough the acquisition and coding process, and the retroduction approach to reflect about causal mechanisms of NVS. The result of this process will be a theoretic model, represented by a Directed Acyclic Graph (DAG), which abstracts relationships of cause between variables.\n\nIn the second part, a sociological analysis describes how groups of interest are related to the variables from the NVS theoretical model, and what are the interactions between different groups, according to their actions and power status. The analysis will be based on Fields Theory of Bourdieu.\n\nThe third part contains the development and employment of statistical regression algorithms, that will be trained and tested using available data that are represented in the NVS theoretic model. The prediction of the models will be analyzed by machine learning interpretation techniques.\n\nThe fourth and last part is where results of both approaches (sociological and statistical analyses) will be interpreted in conjunction.\n\nThe sequence of activities that compose each part are represented in @fig-method-diagram, summarizing the method.\n\n\n```{mermaid}\n%%| label: fig-method-diagram\n%%| fig-cap: \"General workflow of the method. The activities that composes each part of the project are: (1) Literature Assessment, Experts Interviews, Theoretical Model; (2) Sociological Analysis; (3) Available Data, Database Structuring, Statistical Analysis; (4) Results Interpretation.\"\n%%| fig-responsive: true\n%%| fig-align: center\n\nflowchart TD\n    A[Literature Assessment] --> B[Theoretical Model]\n    C[Experts Interviews] --> B\n    B --> D[Database Structuring]\n    E[Available Data] --> D\n    B --> F[Sociological Analysis]\n    C --> G[Statistical Analysis]\n    D --> G\n    F --> H[Results Interpretation]\n    G --> H\n\n```\n\n\nThe combination of activities configures as a Convergent Parallel Mixed Method of research, where quantitative and qualitative analysis are perform at the same time, providing results to a final interpretation, However, the design of this work includes a previous establishment of a common theory that guides both analysis, so they share the same foundations.\n\nIn the next sections, each part of the work, and its respective activities are described.\n\n## Theoretical Model\n\nThe theoretical model will be created by the employment of an adaptation of the Critical Realist Grounded Theory. The method of Grounded Theory is widely used in social sciences with the objective of generating theories, it consists of an interactive and concurrent process of data collection and analysis. It have been modified trough time to accommodate different paradigms of research, such as the Critical Realism.\n\nThe Critical Realist Grounded Theory introduces the necessity to explicitly account for the link between observations and their underlying generative mechanisms, and the use of retroduction in order to abstract these mechanisms [@oliver-2011].\n\nThe process of data acquisition is performed by a series of interviews and literature assessment. The interviews inquire possible causes of NVS and the social groups related to this phenomena. After each interview, the answers are coded into general terms, and with related question of how and why they can affect NVS. These questions are transformed into literature database queries, that helps finding written material that may enrich analysis with convergences or divergences with interview answers, or with new causes of NVS. After each round of interviews and literature assessment, the next interview may change to accommodate findings from the previous. This cyclic process is demonstrated in @fig-theoretical-diagram.\n\n\n```{mermaid}\n%%| label: fig-theoretical-diagram\n%%| fig-cap: Workflow of theoretical model development\n%%| fig-responsive: true\n%%| fig-align: center\n\nflowchart TD\n    A[Interview] --> B[Data]\n    C[Questions] --> D[Literature Assessment]\n    D --> E[Data]\n    B --> F[Theoretical Model]\n    F --> C\n    E --> F\n    F --> A\n\n```\n\n\nThis process is repeated until the data from interviews and literature assessment starts to get redundant, adding little additional information to the theoretical model.\n\n### Experts Interviews\n\nThe objective of the interviews is to explore a diverse range of possible causes of NVS, and its impacts on the environment and society.\n\nThe target participants are researchers with past experience on analyzing NVS in the Amazon or Cerrado biomes.\n\nThe prospecting of potential participants are done by snowball sampling.\n\nThe interviews consist of open and closed questions (see @sec-interview-schedule for the interview schedule), and an interactive activity of building a DAG with the participants (see @sec-interview-dag).\n\n### Literature Assessment\n\nThe literature assessment provides information to the creation of the theoretical model, based on published results. However, it is performed as a complement to the reviews. The queries of the literature assessment are created based on reflections of the causal relationships identified in the interviews (e.g. if variable *A* is believed to cause *B*, what is the chain of events that allows this process?). Once the queries are created, a forward snowball search is performed, in which the resulting papers and its citing documents are screened.\n\n\n```{mermaid}\n%%| label: fig-review-diagram\n%%| fig-cap: Workflow of literature review\n%%| fig-responsive: true\n%%| fig-align: center\n\nflowchart TD\n    A[Questions] --> B[Query Development]\n    A[Questions] --> C[Documents Screening]\n    B[Query Development] --> D[Database Search]\n    D[Database Search] --> C[Documents Screening]\n    C[Documents Screening] --> E[Documents Collection]\n    E[Documents Collection] --> F[Forward Snowball Search]\n    F[Forward Snowball Search] --> C[Documents Screening]\n    E[Documents Collection] --> G[Final Collection]\n\n```\n\n\nThe relevant results from the literature are documented and stored for the creation of the theoretical model, in conjunction with the data acquired from the interviews.\n\n## Sociological Analysis\n\nThe sociological analysis is based on the theoretical model (created from the grounded theory), and it is performed according to the Field Theory of Bourdieu. The analysis consists on defining a social field, its social rules, social groups within the field, and the power relationships between the groups.\n\nIn this project, the social field is idealized as an arena where NVS is materialized, in which I name as Land Use and Cover Field (LUCF). This field is where disputes over land happens. The concept of LUCF is broad, and may contain diverse types of disputes, however, in this work it is bounded to disputes related to NVS.\n\nThe social rules dictate the disputes inside the LUCF, and are not restrained only by the politics institutions and devices.\n\nSocial groups are conceptualized as groups of interest that represents not only individuals that share a common culture and goals, but also institutions.\n\nThe power of the social groups can manifest in different ways, such as economic, political, symbolic, social and cultural power.\n\nThe sociological analysis will consist in identifying the social groups and their power in each of the causes of NVS (that were proposed in the theoretical model). The relationship between these social groups will also be characterized.\n\n::: {.callout-important collapse=\"true\"}\nREAD:\n\n-   <https://doi.org/10.1177/026327696013002001>\n\n-   <https://doi.org/10.2752/152897999786690753>\n\n-   <https://doi.org/10.1162/GLEP_a_00323>\n:::\n\n## Statistical Analysis\n\nThe statistical analysis\n\n### Database Structuring\n\nThe database will contain data from different domains, which can be available in diverse formats and structures. To make the modeling process more efficient and transparent, all variables of interest must be organized coherently.\n\nThe process to create the database will consist of accessing the data sources, downloading the data to the local environment, transforming and organizing the data to the grid (if the data source provides any pre-processing tool before downloading the data, it should be done that way), and store in the local environment.\n\n\n```{mermaid}\n%%| label: fig-data-diagram\n%%| fig-cap: Mermaid diagram\n%%| fig-responsive: true\n%%| fig-align: center\n\nflowchart TD\n    A[Data Source] --> E[Pre Processing]\n    B[Base Grid] --> E[Pre Processing]\n    E[Pre Processing] --> F[Gridded Data]\n    F[Gridded Data] --> G[Database]\n\n```\n\n\nThe data will be stored in a nested directories database with hive-style partitioning. The database will be composed of a collection of parquet files, a format with high compression, that facilitates processing large volumes of data. The main partition will be the variables, which means that each variable will be stored in a separate directory. If necessary, the data will be partitioned by further variables (as a rule of thumb, each parquet file should not be smaller than 20MB and not bigger than 2GB).\n\nTo structure a collection of data that presents different natures, I will transform all the variables into a standard grid system. The grid system will be composed of 0.2 degrees cells (40km x 40km), which should be a balanced compromise between detail and computational cost.\n\n\n::: {.cell fig.asp='1'}\n::: {.cell-output-display}\n![Model grid, in which variables were transformed to](02_methods_files/figure-html/fig-base-grid-1.png){#fig-base-grid width=672}\n:::\n:::\n\n\nEach grid cell will contain an observation, for each year, for all the variables. Only pixels that contained natural vegetation in the first year of the analysis will be considered valid.\n\n### Model Development\n\nEach model will follow the standard approach to develop supervised models.\n\n\n```{mermaid}\n%%| label: fig-model-diagram\n%%| fig-cap: Mermaid diagram\n%%| fig-responsive: true\n%%| fig-align: center\n\nflowchart TD\n    A[Database] --> B[Variables Selection]\n    B[Variables Selection] --> C[Data Split]\n    C[Data Split] --> D[Training Data]\n    D[Training Data] --> E[Pre Processing]\n    E[Pre Processing] --> F[Initial Model Fit]\n    C[Data Split] --> G[Validation Data]\n    C[Data Split] --> H[Test Data]\n    F[Initial Model Fit] --> I[Parameters Adjustments]\n    G[Validation Data] --> I[Parameters Adjustments]\n    I[Parameters Adjustments] --> J[Final Model Fit]\n    H[Test Data] --> K[Model Assessment]\n    J[Final Model Fit] --> K[Model Assessment]\n    J[Final Model Fit] --> L[Model Interpretation]\n    K[Model Assessment] --> L[Model Interpretation]\n\n```\n\n\n# References {.unnumbered}",
    "supporting": [
      "02_methods_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}